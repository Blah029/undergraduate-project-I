{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Conclude setting / general reprocessing / plots / metrices / datasets\n",
    "from utils.utils import \\\n",
    "    time_synchronized,select_device, increment_path,\\\n",
    "    scale_coords,xyxy2xywh,non_max_suppression,split_for_trace_model,\\\n",
    "    driving_area_mask,lane_line_mask,plot_one_box,show_seg_result,\\\n",
    "    AverageMeter,\\\n",
    "    LoadImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default='data/weights/yolopv2.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--source', type=str, default='data/example.jpg', help='source')  # file/folder, 0 for webcam\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.3, help='object confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
    "    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "    parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    return parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GPT-4)\n",
    "\n",
    "This code segment defines a function `make_parser` that creates and returns an instance of `argparse.ArgumentParser`. The `ArgumentParser` object is used to parse command-line arguments. The function adds several arguments to the parser using the `add_argument` method. Each argument has a name, type, default value, and help text. The arguments can be used to specify various options for the program such as the path to the weights file, the source of the input data, the image size, confidence threshold, IOU threshold for NMS, device to use for computation, whether to save confidences and results to text files, whether to save images/videos, classes to filter by, whether to use class-agnostic NMS, and the project and name for saving results. The `exist-ok` argument specifies whether it is okay to overwrite existing project/name or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect():\n",
    "    # setting and directories\n",
    "    source, weights,  save_txt, imgsz = opt.source, opt.weights,  opt.save_txt, opt.img_size\n",
    "    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
    "\n",
    "    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    inf_time = AverageMeter()\n",
    "    waste_time = AverageMeter()\n",
    "    nms_time = AverageMeter()\n",
    "\n",
    "    # Load model\n",
    "    stride =32\n",
    "    model  = torch.jit.load(weights)\n",
    "    device = select_device(opt.device)\n",
    "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "    model = model.to(device)\n",
    "\n",
    "    if half:\n",
    "        model.half()  # to FP16  \n",
    "    model.eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    t0 = time.time()\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        [pred,anchor_grid],seg,ll= model(img)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # waste time: the incompatibility of  torch.jit.trace causes extra time consumption in demo version \n",
    "        # but this problem will not appear in offical version \n",
    "        tw1 = time_synchronized()\n",
    "        pred = split_for_trace_model(pred,anchor_grid)\n",
    "        tw2 = time_synchronized()\n",
    "\n",
    "        # Apply NMS\n",
    "        t3 = time_synchronized()\n",
    "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "        t4 = time_synchronized()\n",
    "\n",
    "        da_seg_mask = driving_area_mask(seg)\n",
    "        ll_seg_mask = lane_line_mask(ll)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "          \n",
    "            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    #s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img :  # Add bbox to image\n",
    "                        plot_one_box(xyxy, im0, line_thickness=3)\n",
    "\n",
    "            # Print time (inference)\n",
    "            print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "            show_seg_result(im0, (da_seg_mask,ll_seg_mask), is_demo=True)\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                    print(f\" The image with the result is saved in: {save_path}\")\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            #w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            #h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                            w,h = im0.shape[1], im0.shape[0]\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                            save_path += '.mp4'\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    inf_time.update(t2-t1,img.size(0))\n",
    "    nms_time.update(t4-t3,img.size(0))\n",
    "    waste_time.update(tw2-tw1,img.size(0))\n",
    "    print('inf : (%.4fs/frame)   nms : (%.4fs/frame)' % (inf_time.avg,nms_time.avg))\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GPT-4)\n",
    "\n",
    "The function `detect` performs object detection on images or videos. It first extracts several options from the global `opt` variable such as the source of the input data, the path to the weights file, whether to save results to text files, and the image size. The `save_img` variable is set to `True` if images should be saved and the input source is not a text file. The `save_dir` variable is set to the path of the directory where results will be saved. The function creates several `AverageMeter` objects to keep track of inference time, waste time, and NMS time.\n",
    "\n",
    "The function then loads the model from the specified weights file using `torch.jit.load` and moves it to the selected device using the `to` method. If the device is not a CPU, the model is converted to half precision using the `half` method. The model is then set to evaluation mode using the `eval` method.\n",
    "\n",
    "The function creates a `LoadImages` object to load images from the specified source. If the device is not a CPU, the function runs the model once on a dummy input to warm up the GPU. The function then enters a loop that iterates over the images in the dataset. For each image, the function converts it to a PyTorch tensor, moves it to the selected device, converts it to half precision if necessary, and normalizes its pixel values to the range [0, 1]. If the image has only 3 dimensions (i.e., it is a single image rather than a batch of images), an extra dimension is added using the `unsqueeze` method.\n",
    "\n",
    "The function then performs inference on the image by passing it through the model. The model returns three outputs: `pred`, `anchor_grid`, and `seg`. The function records the time taken for inference and waste time (the extra time consumption caused by incompatibility of torch.jit.trace in demo version). Finally, `pred` and `anchor_grid` are passed through a function called `split_for_trace_model`.\n",
    "\n",
    "The function applies non-maximum suppression (NMS) to the predicted bounding boxes using the `non_max_suppression` function. The function records the time taken for NMS. The `da_seg_mask` and `ll_seg_mask` variables are set to the outputs of the `driving_area_mask` and `lane_line_mask` functions applied to `seg` and `ll`, respectively.\n",
    "\n",
    "The function then enters a loop that iterates over the predicted bounding boxes for each image. For each image, the function sets several variables such as the path of the image, a string to hold information about the image, and the frame number if the input source is a video. The function constructs the paths of the files where the results will be saved using the `save_dir` and `p` variables. The function also computes a normalization gain tensor `gn` that will be used to normalize bounding box coordinates.\n",
    "\n",
    "If there are any predicted bounding boxes for the current image, the function rescales their coordinates from the size of the input image to the size of the original image using the `scale_coords` function. The function then enters a loop that iterates over the unique class indices in the predicted bounding boxes. For each class index, the function counts the number of predicted bounding boxes with that class index.\n",
    "\n",
    "The function then enters another loop that iterates over each predicted bounding box in reverse order. If results should be saved to text files (`save_txt` is `True`), the function converts the bounding box coordinates from `(x1, y1, x2, y2)` format to `(x_center, y_center, width, height)` format using the `xyxy2xywh` function and normalizes them using the `gn` tensor. The function then constructs a line of text containing either 5 or 6 values depending on whether confidences should be saved or not (`opt.save_conf`). The line is written to a text file at `txt_path`. If images should be saved (`save_img` is `True`), the function draws a bounding box on the image using the `plot_one_box` function.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GPT-3.5)\n",
    "\n",
    "The code then begins by printing the time taken for the inference process, which is the time duration between `t1` and `t2`. It uses string formatting to display the elapsed time in seconds with three decimal places.\n",
    "\n",
    "The `show_seg_result()` function is called to display the segmentation results on the original image (`im0`). The `is_demo` parameter is set to `True`, indicating that this is a demonstration mode.\n",
    "\n",
    "The code checks if `save_img` is set to `True`, indicating that the results with detections need to be saved as an image or a video.\n",
    "\n",
    "If the dataset mode is set to `'image'`, it saves the resulting image (`im0`) at the specified `save_path` using the OpenCV `cv2.imwrite()` function. It then prints the path where the image is saved.\n",
    "\n",
    "If the dataset mode is not `'image'`, which means it's either `'video'` or `'stream'`, the code checks if the `vid_path` is different from the `save_path`. If it is, it releases the previous video writer (if any) using the `release()` method. It then retrieves the frames per second (`fps`), width (`w`), and height (`h`) of the input video (if `vid_cap` is not None) or sets default values for `fps`, `w`, and `h` if `vid_cap` is None, indicating that it's a video stream. It also appends `'.mp4'` to the `save_path` if it's a video stream.\n",
    "\n",
    "Next, the code initializes a new video writer using the `cv2.VideoWriter()` function with the specified `save_path`, `cv2.VideoWriter_fourcc(*'mp4v')` as the codec for writing video in mp4 format, `fps` as the frames per second, and `(w, h)` as the frame size.\n",
    "\n",
    "The resulting image (`im0`) is written to the video writer using the `write()` method of the `vid_writer` object.\n",
    "\n",
    "The code calculates and updates the average inference time per frame (`inf_time.avg`), average non-maximum suppression time per frame (`nms_time.avg`), and average waste time per frame (`waste_time.avg`) based on the time durations between `t1` and `t2`, `t3` and `t4`, and `tw1` and `tw2`, respectively.\n",
    "\n",
    "Finally, the code prints the average inference time per frame (`inf_time.avg`) and average non-maximum suppression time per frame (`nms_time.avg`), and the total time taken for the entire `detect()` function to complete (`time.time() - t0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights='data/weights/yolopv2.pt', source='data/example.jpg', img_size=640, conf_thres=0.3, iou_thres=0.45, device='0', save_conf=False, save_txt=False, nosave=False, classes=None, agnostic_nms=False, project='runs/detect', name='exp', exist_ok=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python\\Python39\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384x640 Done. (2.934s)\n",
      " The image with the result is saved in: runs\\detect\\exp7\\example.jpg\n",
      "inf : (2.9344s/frame)   nms : (0.0695s/frame)\n",
      "Done. (3.241s)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    opt, unknown =  make_parser().parse_known_args()\n",
    "    print(opt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            detect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(GPT-4)\n",
    "\n",
    "This code segment appears to be written in Python. The `if __name__ == '__main__':` statement checks if the script is being run as the main program and not being imported as a module. If it is being run as the main program, the code within the if block will be executed.\n",
    "\n",
    "The `opt, unknown =  make_parser().parse_known_args()` line calls the `make_parser` function and then calls the `parse_known_args` method on the returned object. This method returns a tuple containing two values: `opt` and `unknown`. The `opt` variable contains the parsed command line arguments while `unknown` contains any unrecognized arguments.\n",
    "\n",
    "The `print(opt)` line prints the value of the `opt` variable.\n",
    "\n",
    "The `with torch.no_grad():` statement creates a context in which gradients are not computed by PyTorch. This can be useful when performing inference with a neural network since gradients are not needed in this case.\n",
    "\n",
    "Finally, within this context, the `detect()` function is called."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
